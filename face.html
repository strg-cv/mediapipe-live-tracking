<!DOCTYPE html>
<html lang="de">
<head>
  <meta charset="UTF-8" />
  <title>Live Web Face Tracker</title>

  <style>
    html, body {
      margin: 0;
      padding: 0;
      overflow: hidden;
      background: black;
      font-family: monospace;
      color: #00ffcc;
    }

    video, canvas {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover;
    }

    #hud {
      position: absolute;
      top: 10px;
      left: 10px;
      background: rgba(0,0,0,0.5);
      padding: 10px;
      border-radius: 6px;
      line-height: 1.2em;
    }
  </style>
</head>

<body>
  <video id="video" autoplay playsinline muted></video>
  <canvas id="canvas"></canvas>

  <div id="hud">
    <div id="pose">Pose:</div>
    <div id="eyes">Eyes:</div>
  </div>

  <script type="module">
    import {
      FaceLandmarker,
      FilesetResolver
    } from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3";

    const video = document.getElementById("video");
    const canvas = document.getElementById("canvas");
    const ctx = canvas.getContext("2d");
    const poseDiv = document.getElementById("pose");
    const eyesDiv = document.getElementById("eyes");

    let videoScaleX = 1;
    let videoScaleY = 1;
    let offsetX = 0;
    let offsetY = 0;

    function resizeCanvas() {
      canvas.width = window.innerWidth;
      canvas.height = window.innerHeight;

      const videoAR = video.videoWidth / video.videoHeight;
      const canvasAR = canvas.width / canvas.height;

      if (canvasAR > videoAR) {
        videoScaleX = canvas.width / video.videoWidth;
        videoScaleY = videoScaleX;
        offsetX = 0;
        offsetY = (canvas.height - video.videoHeight * videoScaleY) / 2;
      } else {
        videoScaleY = canvas.height / video.videoHeight;
        videoScaleX = videoScaleY;
        offsetY = 0;
        offsetX = (canvas.width - video.videoWidth * videoScaleX) / 2;
      }
    }

    window.addEventListener("resize", resizeCanvas);

    async function startCamera() {
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { facingMode: "user" }
      });
      video.srcObject = stream;
      await video.play();
      resizeCanvas();
    }

    async function initTracker() {
      const vision = await FilesetResolver.forVisionTasks(
        "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
      );

      return await FaceLandmarker.createFromOptions(vision, {
        baseOptions: {
          modelAssetPath:
            "https://storage.googleapis.com/mediapipe-assets/face_landmarker.task",
          delegate: "CPU"
        },
        runningMode: "VIDEO",
        numFaces: 1
      });
    }

    const dist = (a, b) => Math.hypot(a.x - b.x, a.y - b.y, a.z - b.z);

    function drawLandmarks(landmarks) {
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.fillStyle = "#00ffcc";

      for (const p of landmarks) {
        const x = p.x * video.videoWidth * videoScaleX + offsetX;
        const y = p.y * video.videoHeight * videoScaleY + offsetY;
        ctx.beginPath();
        ctx.arc(x, y, 2, 0, Math.PI * 2);
        ctx.fill();
      }
    }

    function getHeadPose(lm) {
      const nose = lm[1];
      const leftEye = lm[33];
      const rightEye = lm[263];
      const chin = lm[152];

      return {
        yaw: (leftEye.z - rightEye.z) * 100,
        pitch: (nose.y - chin.y) * 180,
        roll: Math.atan2(
          rightEye.y - leftEye.y,
          rightEye.x - leftEye.x
        ) * (180 / Math.PI)
      };
    }

    function getEyeData(lm) {
      return {
        left: dist(lm[159], lm[145]),
        right: dist(lm[386], lm[374])
      };
    }

    function liveLoop(faceLandmarker) {
      if (video.readyState >= 2) {
        const result = faceLandmarker.detectForVideo(video, performance.now());

        if (result.faceLandmarks.length) {
          const lm = result.faceLandmarks[0];

          drawLandmarks(lm);

          const pose = getHeadPose(lm);
          const eyes = getEyeData(lm);

          poseDiv.innerText =
            `Pose\nYaw: ${pose.yaw.toFixed(2)}\nPitch: ${pose.pitch.toFixed(2)}\nRoll: ${pose.roll.toFixed(2)}`;

          eyesDiv.innerText =
            `Eyes\nLeft: ${eyes.left.toFixed(3)}\nRight: ${eyes.right.toFixed(3)}`;
        }
      }
      requestAnimationFrame(() => liveLoop(faceLandmarker));
    }

    (async () => {
      await startCamera();
      const tracker = await initTracker();
      liveLoop(tracker);
    })();
  </script>
</body>
</html>
